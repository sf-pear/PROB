{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FathomNet\n",
    "\n",
    "Training PROB on custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this to create the necessary folder\n",
    "# import os \n",
    "\n",
    "# data_dir = \"./data/OWOD/\"\n",
    "# folders = ['JPEGImages', 'Annotations', 'ImageSets']\n",
    "\n",
    "# for folder in folders:\n",
    "#     try:\n",
    "#         os.makedirs(os.path.join(data_dir, folder, data_name))\n",
    "#     except OSError as e:\n",
    "#         print(f\"Can't create folder: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to add a dataset \"DATASET_A\". Then you need to:\n",
    "\n",
    "1. In `./datasets/torchvision_datasets/open_world.py` line 120, add to the dictionary VOC_COCO_CLASS_NAMES a key-value pair: VOC_COCO_CLASS_NAMES[\"DATASET_A\"]=[\"a\",\"b\",\"c\",...]\n",
    "2. Store DATASET_A's images under \"data/OWOD/JPEGImages/\"\n",
    "3. Store DATASET_A's Annotations under \"data/OWOD/Annotations/\"\n",
    "4. Store DATASET_A's ImageSets files under \"data/OWOD/ImageSets/DATASET_A/\"\n",
    "5. When you train, the input --dataset should be set to DATASET_A (e.g., --dataset DATASET_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "UNK_CLASS = [\"unknown\"]\n",
    "\n",
    "VOC_COCO_CLASS_NAMES = {}\n",
    "\n",
    "T1_CLASS_NAMES = [\n",
    "    'Urchin', 'Fish', 'Sea star', 'Anemone', 'Sea cucumber', \n",
    "    'Sea pen', 'Sea fan', 'Worm', 'Crab', 'Gastropod'\n",
    "]\n",
    "\n",
    "T2_CLASS_NAMES = [\n",
    "    'Shrimp', 'Soft coral'\n",
    "]\n",
    "\n",
    "T3_CLASS_NAMES = [\n",
    "    'Glass sponge', 'Feather star'\n",
    "]\n",
    "\n",
    "T4_CLASS_NAMES = [\n",
    "    'Eel', 'Squat lobster', 'Barnacle', 'Stony coral', 'Black coral', 'Sea spider'\n",
    "]\n",
    "\n",
    "VOC_COCO_CLASS_NAMES[\"fathomnet\"] = tuple(itertools.chain(T1_CLASS_NAMES, T2_CLASS_NAMES, T3_CLASS_NAMES, T4_CLASS_NAMES, UNK_CLASS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_COCO_CLASS_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other files to change:\n",
    "\n",
    "- `configs/M_OWOD_BENCHMARK.sh` update all paths to point to the correct ImageSet files\n",
    "- `run.sh` update the number of GPUs you have in your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort out WANDB:\n",
    "- change entity (aka wandb username) in lines 165 and 167 in the file `/main_open_world.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# confirm login\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are issues with the file names, they have to be numbers like the VOC dataset for some reason. Quicker to do this than change the code.\n",
    "- Annotations and JPEGImages need a new file name,\n",
    "- File paths need to be updated inside all annotation files, and \n",
    "- all txt files inside ImageSets need to be updated to match.\n",
    "\n",
    "If it all goes wrong and all files need to be copied again do it in bash in the `./data/processed` folder `cp -frp VOC-backup -T VOC-test` make sure the taget folder name doesn't exist already ([stackoverflow](https://stackoverflow.com/questions/33343840/bash-duplicate-rename-folder))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `datasets/torchvision_datasets/open_world.py` lines 193 and 198, change .jpg to .png as our images are pngs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "### Compiling CUDA operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd models\n",
    "# !wget https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth\n",
    "# %cd ops\n",
    "# !sh ./make.sh\n",
    "# %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Orr's [reply](https://github.com/orrzohar/PROB/issues/34).\n",
    "\n",
    "Notebooks for ref: \n",
    "- [Objective: fine-tuning DETR](https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb)\n",
    "- [Object Detection with DETR - a minimal implementation](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=kqe_0nc5dyAq)\n",
    "\n",
    "Questions:\n",
    "- how do i load the model from the pre-trained weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.prob_deformable_detr import build\n",
    "from models.deformable_detr import build\n",
    "from models import build_model\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Args:\n",
    "    lr = 2e-4\n",
    "    lr_backbone_names = [\"backbone.0\"]\n",
    "    lr_backbone = 2e-5\n",
    "    lr_linear_proj_names = ['reference_points', 'sampling_offsets']\n",
    "    lr_linear_proj_mult = 0.1\n",
    "    batch_size = 5\n",
    "    weight_decay = 1e-4\n",
    "    epochs = 51\n",
    "    lr_drop = 35\n",
    "    lr_drop_epochs = None\n",
    "    clip_max_norm = 0.1\n",
    "    sgd = False\n",
    "    with_box_refine = False\n",
    "    two_stage = False\n",
    "    masks = False\n",
    "    backbone = 'dino_resnet50'\n",
    "    frozen_weights = None\n",
    "    dilation = False\n",
    "    position_embedding = 'sine'\n",
    "    position_embedding_scale = 2 * np.pi\n",
    "    num_feature_levels = 4\n",
    "    enc_layers = 6\n",
    "    dec_layers = 6\n",
    "    dim_feedforward = 1024\n",
    "    hidden_dim = 256\n",
    "    dropout = 0.1\n",
    "    nheads = 8\n",
    "    num_queries = 100\n",
    "    dec_n_points = 4\n",
    "    enc_n_points = 4\n",
    "    aux_loss = True\n",
    "    set_cost_class = 2\n",
    "    set_cost_bbox = 5\n",
    "    set_cost_giou = 2\n",
    "    cls_loss_coef = 2\n",
    "    bbox_loss_coef = 5\n",
    "    giou_loss_coef = 2\n",
    "    focal_alpha = 0.25\n",
    "    coco_panoptic_path = None\n",
    "    remove_difficult = False\n",
    "    output_dir = ''\n",
    "    device = 'cuda'\n",
    "    seed = 42\n",
    "    resume = './exps/MOWODB/PROB/t1/checkpoint0040.pth'\n",
    "    start_epoch = 0\n",
    "    eval = False\n",
    "    viz = False\n",
    "    eval_every = 5\n",
    "    num_workers = 3\n",
    "    cache_mode = False\n",
    "    PREV_INTRODUCED_CLS = 0\n",
    "    CUR_INTRODUCED_CLS = 10\n",
    "    unmatched_boxes = False\n",
    "    top_unk = 5\n",
    "    featdim = 1024\n",
    "    invalid_cls_logits = False\n",
    "    NC_branch = False\n",
    "    bbox_thresh = 0.3\n",
    "    pretrain = './exps/MOWODB/PROB/t1/checkpoint0040.pth'\n",
    "    nc_loss_coef = 2\n",
    "    train_set = 'task1_train'\n",
    "    test_set = 'all_eval'\n",
    "    num_classes = 21\n",
    "    nc_epoch = 0\n",
    "    dataset = 'fathomnet'\n",
    "    data_root = '/home/sabrina/code/PROB/data/OWOD'\n",
    "    unk_conf_w = 1.0\n",
    "    model_type = 'prob'\n",
    "    wandb_name = ''\n",
    "    wandb_project = 'fathomnet'\n",
    "    obj_loss_coef = 1\n",
    "    obj_temp = 1\n",
    "    freeze_prob_model = False\n",
    "    num_inst_per_class = 50\n",
    "    exemplar_replay_selection = False\n",
    "    exemplar_replay_max_length = 1e10\n",
    "    exemplar_replay_dir = ''\n",
    "    exemplar_replay_prev_file = ''\n",
    "    exemplar_replay_cur_file = ''\n",
    "    exemplar_replay_random = False\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import build_model\n",
    "\n",
    "model, criterion, postprocessors, exemplar_selection = build_model(args, mode=args.model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\":\n",
    "            [p for n, p in model_without_ddp.named_parameters()\n",
    "                if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "    }\n",
    "]\n",
    "\n",
    "if args.sgd:\n",
    "    optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                    weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.torchvision_datasets.open_world import OWDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets.transforms as T\n",
    "\n",
    "def make_coco_transforms(image_set):\n",
    "\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "    t=[]\n",
    "    \n",
    "    if 'train' in image_set:\n",
    "        t.append(['train'])\n",
    "        t.append(T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomSelect(\n",
    "                T.RandomResize(scales, max_size=1333),\n",
    "                T.Compose([\n",
    "                    T.RandomResize([400, 500, 600]),\n",
    "                    T.RandomSizeCrop(384, 600),\n",
    "                    T.RandomResize(scales, max_size=1333),\n",
    "                ])\n",
    "            ),\n",
    "            normalize,\n",
    "        ]))\n",
    "        return t\n",
    "    \n",
    "    if 'ft' in image_set:\n",
    "        t.append(['ft'])\n",
    "        t.append(T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomSelect(\n",
    "                T.RandomResize(scales, max_size=1333),\n",
    "                T.Compose([\n",
    "                    T.RandomResize([400, 500, 600]),\n",
    "                    T.RandomSizeCrop(384, 600),\n",
    "                    T.RandomResize(scales, max_size=1333),\n",
    "                ])\n",
    "            ),\n",
    "            normalize,\n",
    "        ]))\n",
    "        return t\n",
    "\n",
    "    if 'val' in image_set:\n",
    "        t.append(['val'])\n",
    "        t.append(T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            normalize,\n",
    "        ]))\n",
    "        return t\n",
    "\n",
    "    if 'test' in image_set:\n",
    "        t.append(['test'])\n",
    "        t.append(T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            normalize,\n",
    "        ]))\n",
    "        return t\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(args):\n",
    "    print(args.dataset)\n",
    "\n",
    "    train_set = args.train_set\n",
    "    test_set = args.test_set\n",
    "    dataset_train = OWDetection(args, args.data_root, image_set=args.train_set, transforms=make_coco_transforms(args.train_set), dataset = args.dataset)\n",
    "    dataset_val = OWDetection(args, args.data_root, image_set=args.test_set, dataset = args.dataset, transforms=make_coco_transforms(args.test_set))\n",
    "\n",
    "    print(args.train_set)\n",
    "    print(args.test_set)\n",
    "    print(dataset_train)\n",
    "    print(dataset_val)\n",
    "\n",
    "    return dataset_train, dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OWDetection(args, args.data_root, image_set=args.test_set, dataset = args.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = get_datasets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from torch import Tensor\n",
    "\n",
    "from util.misc import NestedTensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
    "    return tuple(batch)\n",
    "\n",
    "\n",
    "def _max_by_axis(the_list):\n",
    "    maxes = the_list[0]\n",
    "    for sublist in the_list[1:]:\n",
    "        for index, item in enumerate(sublist):\n",
    "            maxes[index] = max(maxes[index], item)\n",
    "    return maxes\n",
    "\n",
    "\n",
    "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
    "    # TODO make this more general\n",
    "    if tensor_list[0].ndim == 3:\n",
    "        # TODO make it support different-sized images\n",
    "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
    "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
    "        batch_shape = [len(tensor_list)] + max_size\n",
    "        b, c, h, w = batch_shape\n",
    "        dtype = tensor_list[0].dtype\n",
    "        device = tensor_list[0].device\n",
    "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
    "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
    "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
    "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "            m[: img.shape[1], :img.shape[2]] = False\n",
    "    else:\n",
    "        raise ValueError('not supported')\n",
    "    return NestedTensor(tensor, mask)\n",
    "\n",
    "# class NestedTensor(object):\n",
    "#     def __init__(self, tensors, mask: Optional[Tensor]):\n",
    "#         self.tensors = tensors\n",
    "#         self.mask = mask\n",
    "\n",
    "#     def to(self, device, non_blocking=False):\n",
    "#         cast_tensor = self.tensors.to(device, non_blocking=non_blocking)\n",
    "#         mask = self.mask\n",
    "#         if mask is not None:\n",
    "#             assert mask is not None\n",
    "#             cast_mask = mask.to(device, non_blocking=non_blocking)\n",
    "#         else:\n",
    "#             cast_mask = None\n",
    "#         return NestedTensor(cast_tensor, cast_mask)\n",
    "\n",
    "#     def record_stream(self, *args, **kwargs):\n",
    "#         self.tensors.record_stream(*args, **kwargs)\n",
    "#         if self.mask is not None:\n",
    "#             self.mask.record_stream(*args, **kwargs)\n",
    "\n",
    "#     def decompose(self):\n",
    "#         return self.tensors, self.mask\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return str(self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
    "                                drop_last=False, collate_fn=collate_fn, num_workers=args.num_workers,\n",
    "                                pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ds = dataset_val\n",
    "\n",
    "if args.pretrain:\n",
    "    print('Initialized from the pre-training model')\n",
    "    checkpoint = torch.load(args.pretrain, map_location='cpu')\n",
    "    state_dict = checkpoint['model']\n",
    "    msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "    args.start_epoch = checkpoint['epoch'] + 1\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = engine.evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "        #return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import open_world_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "criterion.eval()\n",
    "# metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "header = 'Test:'\n",
    "iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
    "coco_evaluator = open_world_eval.OWEvaluator(base_ds, iou_types, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.misc import NestedTensor, is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for samples, targets in data_loader_val:\n",
    "    samples = samples.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    outputs = model(samples)\n",
    "    orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "    results = postprocessors['bbox'](outputs, orig_target_sizes)\n",
    "    all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pickle\n",
    "  \n",
    "# # # Open a file and use dump()\n",
    "# # with open('results.pkl', 'wb') as file:\n",
    "      \n",
    "# #     # A new file will be created\n",
    "# #     pickle.dump(results, file)\n",
    "\n",
    "# import pickle\n",
    "  \n",
    "# # Open the file in binary mode\n",
    "# with open('results.pkl', 'rb') as file:\n",
    "      \n",
    "#     # Call load method to deserialze\n",
    "#     results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {i: label for i, label in enumerate(data_loader_val.dataset.CLASS_NAMES, start=0)}\n",
    "\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "def plot_image_with_boxes(target, image_dir, label_map):\n",
    "    # Get the image id and convert it to int\n",
    "    image_id = int(target['image_id'].cpu().item())\n",
    "    \n",
    "    # Construct the image file path\n",
    "    image_path = f'{image_dir}/{image_id}.png'  # adjust the file extension if needed\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Get the image size\n",
    "    img_size = target['size'].cpu().numpy()\n",
    "\n",
    "    # Get the boxes\n",
    "    boxes = target['boxes'].cpu().numpy()\n",
    "\n",
    "    # For each box\n",
    "    for box in boxes:\n",
    "        # Rescale the box\n",
    "        box = box * [img_size[1], img_size[0], img_size[1], img_size[0]]\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "            # Get the label name\n",
    "        label_name = label_map[labels[i]]\n",
    "\n",
    "        # Add the label name\n",
    "        plt.text(box[0], box[1], label_name, fontsize=10, color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "def plot_image_with_boxes(data, image_dir, label_map):\n",
    "    # Get the image id and convert it to int\n",
    "    image_id = int(data['image_id'].cpu().item())\n",
    "    print(image_id)\n",
    "    \n",
    "    # Construct the image file path\n",
    "    image_path = f'{image_dir}/{image_id}.png'  # adjust the file extension if needed\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1, figsize=(15,15))\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Get the image size\n",
    "    img_size = data['size'].cpu().numpy()\n",
    "\n",
    "    # Get the boxes\n",
    "    boxes = data['boxes'].cpu().numpy()\n",
    "\n",
    "    # Get the labels\n",
    "    labels = data['labels'].cpu().numpy()\n",
    "\n",
    "    # For each box\n",
    "    for i, box in enumerate(boxes):\n",
    "        # Rescale the box\n",
    "        box = box * [img_size[1], img_size[0], img_size[1], img_size[0]]\n",
    "\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Get the label name\n",
    "        label_name = label_map[labels[i]]\n",
    "\n",
    "        # Add the label name\n",
    "        plt.text(box[0], box[1], label_name, fontsize=10, color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_with_boxes(targets[1], 'data/OWOD/JPEGImages', label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# def draw_boxes(image_path, boxes, labels, label_map, scores, threshold=0.5):\n",
    "#     \"\"\"\n",
    "#     Draw bounding boxes on an image.\n",
    "    \n",
    "#     Args:\n",
    "#     image_path (str): Path to the image.\n",
    "#     boxes (tensor): Bounding boxes tensor.\n",
    "#     labels (tensor): Labels tensor.\n",
    "#     scores (tensor): Scores tensor.\n",
    "#     threshold (float): Score threshold for displaying bounding boxes.\n",
    "#     \"\"\"\n",
    "#     # Move tensors to CPU and convert to numpy\n",
    "#     boxes = boxes.cpu().numpy()\n",
    "#     labels = labels.cpu().numpy()\n",
    "#     scores = scores.cpu().numpy()\n",
    "\n",
    "#     # Open the image\n",
    "#     im = np.array(Image.open(image_path), dtype=np.uint8)\n",
    "\n",
    "#     # Create figure and axes\n",
    "#     fig, ax = plt.subplots(1, figsize=(15,15))\n",
    "\n",
    "#     # Display the image\n",
    "#     ax.imshow(im)\n",
    "\n",
    "#     # Iterate through the boxes\n",
    "#     for box, label, score in zip(boxes, labels, scores):\n",
    "#         if score > threshold:\n",
    "#             # Create a Rectangle patch\n",
    "#             rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],\n",
    "#                                      linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "#             # Add the patch to the Axes\n",
    "#             ax.add_patch(rect)\n",
    "\n",
    "#             # Get the label name\n",
    "#             label_name = label_map[labels[label]]\n",
    "\n",
    "\n",
    "#             # Add label and score text\n",
    "#             plt.text(box[0], box[1], f'{label_name}: {score:.2f}', \n",
    "#                      color='white', fontsize=10,\n",
    "#                      bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_path, boxes, labels, label_map, scores, threshold=0.5, background_color='#080e26'):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "    image_path (str): Path to the image.\n",
    "    boxes (tensor): Bounding boxes tensor.\n",
    "    labels (tensor): Labels tensor.\n",
    "    scores (tensor): Scores tensor.\n",
    "    threshold (float): Score threshold for displaying bounding boxes.\n",
    "    background_color (str): Background color of the plot.\n",
    "    \"\"\"\n",
    "    # Move tensors to CPU and convert to numpy\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    scores = scores.cpu().numpy()\n",
    "\n",
    "    # Open the image\n",
    "    im = np.array(Image.open(image_path), dtype=np.uint8)\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1, figsize=(15,15))\n",
    "\n",
    "    # Set background color and remove axes\n",
    "    ax.set_facecolor(background_color)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # Iterate through the boxes\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],\n",
    "                                     linewidth=1,edgecolor='#9413C1',facecolor='none')\n",
    "\n",
    "            # Add the patch to the Axes\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Get the label name\n",
    "            label_name = label_map[labels[label]]\n",
    "\n",
    "            # Add label and score text\n",
    "            plt.text(box[0], box[1], f'{label_name}: {score:.2f}', \n",
    "                     color='white', fontsize=10,\n",
    "                     bbox=dict(facecolor='#9413C1', alpha=0.5))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index = 1\n",
    "image_dir = 'data/OWOD/JPEGImages'\n",
    "image_ext = '.png'\n",
    "image_path = f'{image_dir}/{4623}{image_ext}'\n",
    "boxes = results[target_index]['boxes']\n",
    "labels = results[target_index]['labels']\n",
    "scores = results[target_index]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(image_path, boxes, labels, label_map, scores, threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def draw_boxes(image_tensor, boxes, labels, scores, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image.\n",
    "    \n",
    "    Args:\n",
    "    image_tensor (tensor): Tensor representation of the image.\n",
    "    boxes (tensor): Bounding boxes tensor.\n",
    "    labels (tensor): Labels tensor.\n",
    "    scores (tensor): Scores tensor.\n",
    "    threshold (float): Score threshold for displaying bounding boxes.\n",
    "    \"\"\"\n",
    "    # Move tensors to CPU and convert to numpy\n",
    "    image = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    scores = scores.cpu().numpy()\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Iterate through the boxes\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > threshold:\n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],\n",
    "                                     linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "            # Add the patch to the Axes\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add label and score text\n",
    "            plt.text(box[0], box[1], f'{label}: {score:.2f}', \n",
    "                     color='white', fontsize=10,\n",
    "                     bbox=dict(facecolor='red', alpha=0.2))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val.dataset.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val.dataset.CLASS_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index = 15\n",
    "image = data_loader_val.dataset[target_index][0]\n",
    "boxes = results[target_index]['boxes']\n",
    "labels = results[target_index]['labels']\n",
    "scores = results[target_index]['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(image, boxes, labels, scores, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update XML files\n",
    "Also in script `update_xml.py` - still needs a main.\n",
    "\n",
    "### Add missing tags to XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def replace_in_file(file_path, pattern, replacement):\n",
    "    with open(file_path, 'r+') as file:\n",
    "        file_content = file.read()\n",
    "        file_content = re.sub(pattern, replacement, file_content)\n",
    "        file.seek(0)\n",
    "        file.write(file_content)\n",
    "        file.truncate()\n",
    "\n",
    "def replace_in_all_files(directory, pattern, replacement):\n",
    "    for foldername, subfolders, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            replace_in_file(file_path, pattern, replacement)\n",
    "\n",
    "directory = \"./data/OWOD/Annotations/\"\n",
    "pattern = \"/name>\\n        <bndbox>\"\n",
    "replacement = \"/name>\\n        <truncated>0</truncated>\\n        <difficult>0</difficult>\\n        <bndbox>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_in_all_files(directory, pattern, replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update annotation files with new path and new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "dict_csv = \"./data/OWOD/filename_map.csv\"\n",
    "xml_dir = \"./data/OWOD/Annotations/\"\n",
    "new_path_prefix = \"/home/sabrina/code/PROB/data/OWOD/ImageSets/\"\n",
    "\n",
    "def main(dict_csv, xml_dir, new_path_prefix):\n",
    "    logging.basicConfig(filename='xml_update.log', level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(dict_csv)\n",
    "    name_dict = df.set_index('old_name')['new_name'].to_dict()\n",
    "\n",
    "    xml_files = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "\n",
    "    for xml_file in tqdm(xml_files, desc=\"Updating XML files\"):\n",
    "        tree = ET.parse(os.path.join(xml_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for elem in root.iter():\n",
    "            try:\n",
    "                if elem.tag == 'filename':\n",
    "                    old_filename = elem.text.split('.')[0]\n",
    "                    file_extension = elem.text.split('.')[1]\n",
    "\n",
    "                    new_filename = f\"{name_dict[old_filename]}.{file_extension}\"\n",
    "                    new_path = f\"{new_path_prefix}{new_filename}\"\n",
    "\n",
    "                    elem.text = new_filename\n",
    "\n",
    "                if elem.tag == 'path':\n",
    "                    elem.text = new_path\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing XML file {xml_file}: {e}\")\n",
    "                pass\n",
    "\n",
    "        tree.write(os.path.join(xml_dir, xml_file))                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main(dict_csv, xml_dir, new_path_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
