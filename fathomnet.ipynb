{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FathomNet\n",
    "\n",
    "Training PROB on custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"fathomnet\" # give your dataset a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this to create the necessary folder\n",
    "# import os \n",
    "\n",
    "# data_dir = \"./data/OWOD/\"\n",
    "# folders = ['JPEGImages', 'Annotations', 'ImageSets']\n",
    "\n",
    "# for folder in folders:\n",
    "#     try:\n",
    "#         os.makedirs(os.path.join(data_dir, folder, data_name))\n",
    "#     except OSError as e:\n",
    "#         print(f\"Can't create folder: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to add a dataset \"DATASET_A\". Then you need to:\n",
    "\n",
    "1. In `./datasets/torchvision_datasets/open_world.py` line 120, add to the dictionary VOC_COCO_CLASS_NAMES a key-value pair: VOC_COCO_CLASS_NAMES[\"DATASET_A\"]=[\"a\",\"b\",\"c\",...]\n",
    "2. Store DATASET_A's images under \"data/OWOD/JPEGImages/DATASET_A/\"\n",
    "3. Store DATASET_A's Annotations under \"data/OWOD/Annotations/DATASET_A/\"\n",
    "4. Store DATASET_A's ImageSets files under \"data/OWOD/ImageSets/DATASET_A/\"\n",
    "5. When you train, the input --dataset should be set to DATASET_A (e.g., --dataset DATASET_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "UNK_CLASS = [\"unknown\"]\n",
    "\n",
    "VOC_COCO_CLASS_NAMES = {}\n",
    "\n",
    "T1_CLASS_NAMES = [\n",
    "    'Urchin', 'Fish', 'Sea star', 'Anemone', 'Sea cucumber', \n",
    "    'Sea pen', 'Sea fan', 'Worm', 'Crab', 'Gastropod'\n",
    "]\n",
    "\n",
    "T2_CLASS_NAMES = [\n",
    "    'Shrimp', 'Soft coral'\n",
    "]\n",
    "\n",
    "T3_CLASS_NAMES = [\n",
    "    'Glass sponge', 'Feather star'\n",
    "]\n",
    "\n",
    "T4_CLASS_NAMES = [\n",
    "    'Eel', 'Squat lobster', 'Barnacle', 'Stony coral', 'Black coral', 'Sea spider'\n",
    "]\n",
    "\n",
    "VOC_COCO_CLASS_NAMES[\"fathomnet\"] = tuple(itertools.chain(T1_CLASS_NAMES, T2_CLASS_NAMES, T3_CLASS_NAMES, T4_CLASS_NAMES, UNK_CLASS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fathomnet': ('Urchin',\n",
       "  'Fish',\n",
       "  'Sea star',\n",
       "  'Anemone',\n",
       "  'Sea cucumber',\n",
       "  'Sea pen',\n",
       "  'Sea fan',\n",
       "  'Worm',\n",
       "  'Crab',\n",
       "  'Gastropod',\n",
       "  'Shrimp',\n",
       "  'Soft coral',\n",
       "  'Glass sponge',\n",
       "  'Feather star',\n",
       "  'Eel',\n",
       "  'Squat lobster',\n",
       "  'Barnacle',\n",
       "  'Stony coral',\n",
       "  'Black coral',\n",
       "  'Sea spider',\n",
       "  'unknown')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOC_COCO_CLASS_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other files to change:\n",
    "\n",
    "- `configs/M_OWOD_BENCHMARK.sh` update all paths to point to the correct ImageSet files\n",
    "- `/home/sabrina/code/PROB/run.sh` update the number of GPUs you have in your machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort out WANDB:\n",
    "- change entity (aka wandb username) in lines 165 and 167 in the file `/main_open_world.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mspereira\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# confirm login\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are issues with the file names, they have to be numbers like the VOC dataset for some reason. I made a VOC-test folder with all the file sthat need to be changed.\n",
    "- Annotations and JPEGImages need a new file name, and \n",
    "- all txt files inside ImageSets need to be updated to match.\n",
    "\n",
    "If it all goes wrong and all files need to be copied again do it in bash in the `./data/processed` folder `cp -frp VOC-backup -T VOC-test` make sure the taget folder name doesn't exist already ([stackoverflow](https://stackoverflow.com/questions/33343840/bash-duplicate-rename-folder))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `datasets/torchvision_datasets/open_world.py` lines 193 and 198, change .jpg to .png as our images are pngs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "### Compiling CUDA operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd models\n",
    "# !wget https://dl.fbaipublicfiles.com/dino/dino_resnet50_pretrain/dino_resnet50_pretrain.pth\n",
    "# %cd ops\n",
    "# !sh ./make.sh\n",
    "# %cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Orr's [reply](https://github.com/orrzohar/PROB/issues/34).\n",
    "\n",
    "Notebooks for ref: \n",
    "- [Objective: fine-tuning DETR](https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb)\n",
    "- [Object Detection with DETR - a minimal implementation](https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=kqe_0nc5dyAq)\n",
    "\n",
    "Questions:\n",
    "- how do i load the model from the pre-trained weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabrina/mambaforge/envs/prob2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from models.prob_deformable_detr import build\n",
    "from models.deformable_detr import build\n",
    "from models import build_model\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    lr = 2e-4\n",
    "    lr_backbone_names = [\"backbone.0\"]\n",
    "    lr_backbone = 2e-5\n",
    "    lr_linear_proj_names = ['reference_points', 'sampling_offsets']\n",
    "    lr_linear_proj_mult = 0.1\n",
    "    batch_size = 5\n",
    "    weight_decay = 1e-4\n",
    "    epochs = 51\n",
    "    lr_drop = 35\n",
    "    lr_drop_epochs = None\n",
    "    clip_max_norm = 0.1\n",
    "    sgd = False\n",
    "    with_box_refine = False\n",
    "    two_stage = False\n",
    "    masks = False\n",
    "    backbone = 'dino_resnet50'\n",
    "    frozen_weights = None\n",
    "    dilation = False\n",
    "    position_embedding = 'sine'\n",
    "    position_embedding_scale = 2 * np.pi\n",
    "    num_feature_levels = 4\n",
    "    enc_layers = 6\n",
    "    dec_layers = 6\n",
    "    dim_feedforward = 1024\n",
    "    hidden_dim = 256\n",
    "    dropout = 0.1\n",
    "    nheads = 8\n",
    "    num_queries = 100\n",
    "    dec_n_points = 4\n",
    "    enc_n_points = 4\n",
    "    aux_loss = True\n",
    "    set_cost_class = 2\n",
    "    set_cost_bbox = 5\n",
    "    set_cost_giou = 2\n",
    "    cls_loss_coef = 2\n",
    "    bbox_loss_coef = 5\n",
    "    giou_loss_coef = 2\n",
    "    focal_alpha = 0.25\n",
    "    coco_panoptic_path = None\n",
    "    remove_difficult = False\n",
    "    output_dir = ''\n",
    "    device = 'cuda'\n",
    "    seed = 42\n",
    "    resume = './exps/MOWODB/PROB/t1/checkpoint0040.pth'\n",
    "    start_epoch = 0\n",
    "    eval = True\n",
    "    viz = False\n",
    "    eval_every = 5\n",
    "    num_workers = 3\n",
    "    cache_mode = False\n",
    "    PREV_INTRODUCED_CLS = 0\n",
    "    CUR_INTRODUCED_CLS = 10\n",
    "    unmatched_boxes = False\n",
    "    top_unk = 5\n",
    "    featdim = 1024\n",
    "    invalid_cls_logits = False\n",
    "    NC_branch = False\n",
    "    bbox_thresh = 0.3\n",
    "    pretrain = './exps/MOWODB/PROB/t1/checkpoint0039.pth'\n",
    "    nc_loss_coef = 2\n",
    "    train_set = ''\n",
    "    test_set = ''\n",
    "    num_classes = 21\n",
    "    nc_epoch = 0\n",
    "    dataset = 'fathomnet'\n",
    "    data_root = '/home/sabrina/code/PROB/data/OWOD'\n",
    "    unk_conf_w = 1.0\n",
    "    model_type = 'prob'\n",
    "    wandb_name = ''\n",
    "    wandb_project = 'fathomnet'\n",
    "    obj_loss_coef = 1\n",
    "    obj_temp = 1\n",
    "    freeze_prob_model = False\n",
    "    num_inst_per_class = 50\n",
    "    exemplar_replay_selection = False\n",
    "    exemplar_replay_max_length = 1e10\n",
    "    exemplar_replay_dir = ''\n",
    "    exemplar_replay_prev_file = ''\n",
    "    exemplar_replay_cur_file = ''\n",
    "    exemplar_replay_random = False\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid class range: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "DINO resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabrina/mambaforge/envs/prob2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sabrina/mambaforge/envs/prob2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running with exemplar_replay_selection\n"
     ]
    }
   ],
   "source": [
    "model, criterion, postprocessors, exemplar_selection = build_model(args, mode=args.model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_without_ddp = model\n",
    "\n",
    "def match_name_keywords(n, name_keywords):\n",
    "    out = False\n",
    "    for b in name_keywords:\n",
    "        if b in n:\n",
    "            out = True\n",
    "            break\n",
    "    return out\n",
    "\n",
    "param_dicts = [\n",
    "    {\n",
    "        \"params\":\n",
    "            [p for n, p in model_without_ddp.named_parameters()\n",
    "                if not match_name_keywords(n, args.lr_backbone_names) and not match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_backbone_names) and p.requires_grad],\n",
    "        \"lr\": args.lr_backbone,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model_without_ddp.named_parameters() if match_name_keywords(n, args.lr_linear_proj_names) and p.requires_grad],\n",
    "        \"lr\": args.lr * args.lr_linear_proj_mult,\n",
    "    }\n",
    "]\n",
    "\n",
    "if args.sgd:\n",
    "    optimizer = torch.optim.SGD(param_dicts, lr=args.lr, momentum=0.9,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
    "                                    weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized from the pre-training model\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m args\u001b[39m.\u001b[39mstart_epoch \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39meval:\n\u001b[0;32m----> 9\u001b[0m     test_stats, coco_evaluator \u001b[39m=\u001b[39m evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args\u001b[39m.\u001b[39moutput_dir, args)\n\u001b[1;32m     10\u001b[0m     \u001b[39m#return\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if args.pretrain:\n",
    "    print('Initialized from the pre-training model')\n",
    "    checkpoint = torch.load(args.pretrain, map_location='cpu')\n",
    "    state_dict = checkpoint['model']\n",
    "    msg = model_without_ddp.load_state_dict(state_dict, strict=False)\n",
    "    print(msg)\n",
    "    args.start_epoch = checkpoint['epoch'] + 1\n",
    "    if args.eval:\n",
    "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "        #return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DeformableDETR:\n\tsize mismatch for query_embed.weight: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(args\u001b[39m.\u001b[39mresume, map_location\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m missing_keys, unexpected_keys \u001b[39m=\u001b[39m model_without_ddp\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m], strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m unexpected_keys \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m unexpected_keys \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (k\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39mtotal_params\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m k\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39mtotal_ops\u001b[39m\u001b[39m'\u001b[39m))]\n\u001b[1;32m      8\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/prob2/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeformableDETR:\n\tsize mismatch for query_embed.weight: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 512])."
     ]
    }
   ],
   "source": [
    "    if args.resume.startswith('https'):\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(\n",
    "            args.resume, map_location='cpu', check_hash=True)\n",
    "    else:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    missing_keys, unexpected_keys = model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
    "    unexpected_keys = [k for k in unexpected_keys if not (k.endswith('total_params') or k.endswith('total_ops'))]\n",
    "    if len(missing_keys) > 0:\n",
    "        print('Missing Keys: {}'.format(missing_keys))\n",
    "    if len(unexpected_keys) > 0:\n",
    "        print('Unexpected Keys: {}'.format(unexpected_keys))\n",
    "    # if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
    "    #     import copy\n",
    "    #     p_groups = copy.deepcopy(optimizer.param_groups)\n",
    "    #     optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    #     for pg, pg_old in zip(optimizer.param_groups, p_groups):\n",
    "    #         pg['lr'] = pg_old['lr']\n",
    "    #         pg['initial_lr'] = pg_old['initial_lr']\n",
    "    #     print(optimizer.param_groups)\n",
    "    #     lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    #     # todo: this is a hack for doing experiment that resume from checkpoint and also modify lr scheduler (e.g., decrease lr in advance).\n",
    "    #     args.override_resumed_lr_drop = True\n",
    "    #     if args.override_resumed_lr_drop:\n",
    "    #         print('Warning: (hack) args.override_resumed_lr_drop is set to True, so args.lr_drop would override lr_drop in resumed lr_scheduler.')\n",
    "    #         lr_scheduler.step_size = args.lr_drop\n",
    "    #         lr_scheduler.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "    #     lr_scheduler.step(lr_scheduler.last_epoch)\n",
    "    #     args.start_epoch = checkpoint['epoch'] + 1\n",
    "    # check the resumed model\n",
    "    # if (not args.eval and not args.viz and args.dataset in ['coco', 'voc']):\n",
    "    #     test_stats, coco_evaluator = evaluate(\n",
    "    #         model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args\n",
    "    #     )\n",
    "    # if args.eval:\n",
    "    #     test_stats, coco_evaluator = evaluate(model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, args)\n",
    "    #     if args.output_dir:\n",
    "    #         utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
    "    #     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(open('./exps/MOWODB/PROB/t1/checkpoint0040.pth', 'rb'), map_location='cpu')\n",
    "\n",
    "# for i in state_dict['model'].keys():\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DeformableDETR:\n\tsize mismatch for transformer.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.1.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.2.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.2.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.3.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.3.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.3.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.4.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.4.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.4.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.5.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.5.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.5.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.1.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.2.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.2.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.3.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.3.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.3.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.4.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.4.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.4.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.5.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.5.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.5.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for query_embed.weight: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m missing_keys, unexpected_keys \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mload_state_dict(state_dict[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m], strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m \u001b[39m#model.eval()\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/prob2/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeformableDETR:\n\tsize mismatch for transformer.encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.0.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.1.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.2.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.2.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.3.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.3.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.3.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.4.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.4.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.4.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.encoder.layers.5.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.encoder.layers.5.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.encoder.layers.5.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.0.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.1.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.2.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.2.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.2.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.3.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.3.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.3.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.4.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.4.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.4.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for transformer.decoder.layers.5.linear1.weight: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for transformer.decoder.layers.5.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for transformer.decoder.layers.5.linear2.weight: copying a param with shape torch.Size([256, 2048]) from checkpoint, the shape in current model is torch.Size([256, 1024]).\n\tsize mismatch for query_embed.weight: copying a param with shape torch.Size([100, 256]) from checkpoint, the shape in current model is torch.Size([100, 512])."
     ]
    }
   ],
   "source": [
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict['model'], strict=False)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.encoder.layers.0.self_attn.in_proj_weight', 'transformer.encoder.layers.0.self_attn.in_proj_bias', 'transformer.encoder.layers.0.self_attn.out_proj.weight', 'transformer.encoder.layers.0.self_attn.out_proj.bias', 'transformer.encoder.layers.0.linear1.weight', 'transformer.encoder.layers.0.linear1.bias', 'transformer.encoder.layers.0.linear2.weight', 'transformer.encoder.layers.0.linear2.bias', 'transformer.encoder.layers.0.norm1.weight', 'transformer.encoder.layers.0.norm1.bias', 'transformer.encoder.layers.0.norm2.weight', 'transformer.encoder.layers.0.norm2.bias', 'transformer.encoder.layers.1.self_attn.in_proj_weight', 'transformer.encoder.layers.1.self_attn.in_proj_bias', 'transformer.encoder.layers.1.self_attn.out_proj.weight', 'transformer.encoder.layers.1.self_attn.out_proj.bias', 'transformer.encoder.layers.1.linear1.weight', 'transformer.encoder.layers.1.linear1.bias', 'transformer.encoder.layers.1.linear2.weight', 'transformer.encoder.layers.1.linear2.bias', 'transformer.encoder.layers.1.norm1.weight', 'transformer.encoder.layers.1.norm1.bias', 'transformer.encoder.layers.1.norm2.weight', 'transformer.encoder.layers.1.norm2.bias', 'transformer.encoder.layers.2.self_attn.in_proj_weight', 'transformer.encoder.layers.2.self_attn.in_proj_bias', 'transformer.encoder.layers.2.self_attn.out_proj.weight', 'transformer.encoder.layers.2.self_attn.out_proj.bias', 'transformer.encoder.layers.2.linear1.weight', 'transformer.encoder.layers.2.linear1.bias', 'transformer.encoder.layers.2.linear2.weight', 'transformer.encoder.layers.2.linear2.bias', 'transformer.encoder.layers.2.norm1.weight', 'transformer.encoder.layers.2.norm1.bias', 'transformer.encoder.layers.2.norm2.weight', 'transformer.encoder.layers.2.norm2.bias', 'transformer.encoder.layers.3.self_attn.in_proj_weight', 'transformer.encoder.layers.3.self_attn.in_proj_bias', 'transformer.encoder.layers.3.self_attn.out_proj.weight', 'transformer.encoder.layers.3.self_attn.out_proj.bias', 'transformer.encoder.layers.3.linear1.weight', 'transformer.encoder.layers.3.linear1.bias', 'transformer.encoder.layers.3.linear2.weight', 'transformer.encoder.layers.3.linear2.bias', 'transformer.encoder.layers.3.norm1.weight', 'transformer.encoder.layers.3.norm1.bias', 'transformer.encoder.layers.3.norm2.weight', 'transformer.encoder.layers.3.norm2.bias', 'transformer.encoder.layers.4.self_attn.in_proj_weight', 'transformer.encoder.layers.4.self_attn.in_proj_bias', 'transformer.encoder.layers.4.self_attn.out_proj.weight', 'transformer.encoder.layers.4.self_attn.out_proj.bias', 'transformer.encoder.layers.4.linear1.weight', 'transformer.encoder.layers.4.linear1.bias', 'transformer.encoder.layers.4.linear2.weight', 'transformer.encoder.layers.4.linear2.bias', 'transformer.encoder.layers.4.norm1.weight', 'transformer.encoder.layers.4.norm1.bias', 'transformer.encoder.layers.4.norm2.weight', 'transformer.encoder.layers.4.norm2.bias', 'transformer.encoder.layers.5.self_attn.in_proj_weight', 'transformer.encoder.layers.5.self_attn.in_proj_bias', 'transformer.encoder.layers.5.self_attn.out_proj.weight', 'transformer.encoder.layers.5.self_attn.out_proj.bias', 'transformer.encoder.layers.5.linear1.weight', 'transformer.encoder.layers.5.linear1.bias', 'transformer.encoder.layers.5.linear2.weight', 'transformer.encoder.layers.5.linear2.bias', 'transformer.encoder.layers.5.norm1.weight', 'transformer.encoder.layers.5.norm1.bias', 'transformer.encoder.layers.5.norm2.weight', 'transformer.encoder.layers.5.norm2.bias', 'transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.layers.1.self_attn.in_proj_weight', 'transformer.decoder.layers.1.self_attn.in_proj_bias', 'transformer.decoder.layers.1.self_attn.out_proj.weight', 'transformer.decoder.layers.1.self_attn.out_proj.bias', 'transformer.decoder.layers.1.multihead_attn.in_proj_weight', 'transformer.decoder.layers.1.multihead_attn.in_proj_bias', 'transformer.decoder.layers.1.multihead_attn.out_proj.weight', 'transformer.decoder.layers.1.multihead_attn.out_proj.bias', 'transformer.decoder.layers.1.linear1.weight', 'transformer.decoder.layers.1.linear1.bias', 'transformer.decoder.layers.1.linear2.weight', 'transformer.decoder.layers.1.linear2.bias', 'transformer.decoder.layers.1.norm1.weight', 'transformer.decoder.layers.1.norm1.bias', 'transformer.decoder.layers.1.norm2.weight', 'transformer.decoder.layers.1.norm2.bias', 'transformer.decoder.layers.1.norm3.weight', 'transformer.decoder.layers.1.norm3.bias', 'transformer.decoder.layers.2.self_attn.in_proj_weight', 'transformer.decoder.layers.2.self_attn.in_proj_bias', 'transformer.decoder.layers.2.self_attn.out_proj.weight', 'transformer.decoder.layers.2.self_attn.out_proj.bias', 'transformer.decoder.layers.2.multihead_attn.in_proj_weight', 'transformer.decoder.layers.2.multihead_attn.in_proj_bias', 'transformer.decoder.layers.2.multihead_attn.out_proj.weight', 'transformer.decoder.layers.2.multihead_attn.out_proj.bias', 'transformer.decoder.layers.2.linear1.weight', 'transformer.decoder.layers.2.linear1.bias', 'transformer.decoder.layers.2.linear2.weight', 'transformer.decoder.layers.2.linear2.bias', 'transformer.decoder.layers.2.norm1.weight', 'transformer.decoder.layers.2.norm1.bias', 'transformer.decoder.layers.2.norm2.weight', 'transformer.decoder.layers.2.norm2.bias', 'transformer.decoder.layers.2.norm3.weight', 'transformer.decoder.layers.2.norm3.bias', 'transformer.decoder.layers.3.self_attn.in_proj_weight', 'transformer.decoder.layers.3.self_attn.in_proj_bias', 'transformer.decoder.layers.3.self_attn.out_proj.weight', 'transformer.decoder.layers.3.self_attn.out_proj.bias', 'transformer.decoder.layers.3.multihead_attn.in_proj_weight', 'transformer.decoder.layers.3.multihead_attn.in_proj_bias', 'transformer.decoder.layers.3.multihead_attn.out_proj.weight', 'transformer.decoder.layers.3.multihead_attn.out_proj.bias', 'transformer.decoder.layers.3.linear1.weight', 'transformer.decoder.layers.3.linear1.bias', 'transformer.decoder.layers.3.linear2.weight', 'transformer.decoder.layers.3.linear2.bias', 'transformer.decoder.layers.3.norm1.weight', 'transformer.decoder.layers.3.norm1.bias', 'transformer.decoder.layers.3.norm2.weight', 'transformer.decoder.layers.3.norm2.bias', 'transformer.decoder.layers.3.norm3.weight', 'transformer.decoder.layers.3.norm3.bias', 'transformer.decoder.layers.4.self_attn.in_proj_weight', 'transformer.decoder.layers.4.self_attn.in_proj_bias', 'transformer.decoder.layers.4.self_attn.out_proj.weight', 'transformer.decoder.layers.4.self_attn.out_proj.bias', 'transformer.decoder.layers.4.multihead_attn.in_proj_weight', 'transformer.decoder.layers.4.multihead_attn.in_proj_bias', 'transformer.decoder.layers.4.multihead_attn.out_proj.weight', 'transformer.decoder.layers.4.multihead_attn.out_proj.bias', 'transformer.decoder.layers.4.linear1.weight', 'transformer.decoder.layers.4.linear1.bias', 'transformer.decoder.layers.4.linear2.weight', 'transformer.decoder.layers.4.linear2.bias', 'transformer.decoder.layers.4.norm1.weight', 'transformer.decoder.layers.4.norm1.bias', 'transformer.decoder.layers.4.norm2.weight', 'transformer.decoder.layers.4.norm2.bias', 'transformer.decoder.layers.4.norm3.weight', 'transformer.decoder.layers.4.norm3.bias', 'transformer.decoder.layers.5.self_attn.in_proj_weight', 'transformer.decoder.layers.5.self_attn.in_proj_bias', 'transformer.decoder.layers.5.self_attn.out_proj.weight', 'transformer.decoder.layers.5.self_attn.out_proj.bias', 'transformer.decoder.layers.5.multihead_attn.in_proj_weight', 'transformer.decoder.layers.5.multihead_attn.in_proj_bias', 'transformer.decoder.layers.5.multihead_attn.out_proj.weight', 'transformer.decoder.layers.5.multihead_attn.out_proj.bias', 'transformer.decoder.layers.5.linear1.weight', 'transformer.decoder.layers.5.linear1.bias', 'transformer.decoder.layers.5.linear2.weight', 'transformer.decoder.layers.5.linear2.bias', 'transformer.decoder.layers.5.norm1.weight', 'transformer.decoder.layers.5.norm1.bias', 'transformer.decoder.layers.5.norm2.weight', 'transformer.decoder.layers.5.norm2.bias', 'transformer.decoder.layers.5.norm3.weight', 'transformer.decoder.layers.5.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'bbox_embed.layers.0.weight', 'bbox_embed.layers.0.bias', 'bbox_embed.layers.1.weight', 'bbox_embed.layers.1.bias', 'bbox_embed.layers.2.weight', 'bbox_embed.layers.2.bias', 'query_embed.weight', 'input_proj.weight', 'input_proj.bias', 'backbone.0.body.conv1.weight', 'backbone.0.body.bn1.weight', 'backbone.0.body.bn1.bias', 'backbone.0.body.bn1.running_mean', 'backbone.0.body.bn1.running_var', 'backbone.0.body.layer1.0.conv1.weight', 'backbone.0.body.layer1.0.bn1.weight', 'backbone.0.body.layer1.0.bn1.bias', 'backbone.0.body.layer1.0.bn1.running_mean', 'backbone.0.body.layer1.0.bn1.running_var', 'backbone.0.body.layer1.0.conv2.weight', 'backbone.0.body.layer1.0.bn2.weight', 'backbone.0.body.layer1.0.bn2.bias', 'backbone.0.body.layer1.0.bn2.running_mean', 'backbone.0.body.layer1.0.bn2.running_var', 'backbone.0.body.layer1.0.conv3.weight', 'backbone.0.body.layer1.0.bn3.weight', 'backbone.0.body.layer1.0.bn3.bias', 'backbone.0.body.layer1.0.bn3.running_mean', 'backbone.0.body.layer1.0.bn3.running_var', 'backbone.0.body.layer1.0.downsample.0.weight', 'backbone.0.body.layer1.0.downsample.1.weight', 'backbone.0.body.layer1.0.downsample.1.bias', 'backbone.0.body.layer1.0.downsample.1.running_mean', 'backbone.0.body.layer1.0.downsample.1.running_var', 'backbone.0.body.layer1.1.conv1.weight', 'backbone.0.body.layer1.1.bn1.weight', 'backbone.0.body.layer1.1.bn1.bias', 'backbone.0.body.layer1.1.bn1.running_mean', 'backbone.0.body.layer1.1.bn1.running_var', 'backbone.0.body.layer1.1.conv2.weight', 'backbone.0.body.layer1.1.bn2.weight', 'backbone.0.body.layer1.1.bn2.bias', 'backbone.0.body.layer1.1.bn2.running_mean', 'backbone.0.body.layer1.1.bn2.running_var', 'backbone.0.body.layer1.1.conv3.weight', 'backbone.0.body.layer1.1.bn3.weight', 'backbone.0.body.layer1.1.bn3.bias', 'backbone.0.body.layer1.1.bn3.running_mean', 'backbone.0.body.layer1.1.bn3.running_var', 'backbone.0.body.layer1.2.conv1.weight', 'backbone.0.body.layer1.2.bn1.weight', 'backbone.0.body.layer1.2.bn1.bias', 'backbone.0.body.layer1.2.bn1.running_mean', 'backbone.0.body.layer1.2.bn1.running_var', 'backbone.0.body.layer1.2.conv2.weight', 'backbone.0.body.layer1.2.bn2.weight', 'backbone.0.body.layer1.2.bn2.bias', 'backbone.0.body.layer1.2.bn2.running_mean', 'backbone.0.body.layer1.2.bn2.running_var', 'backbone.0.body.layer1.2.conv3.weight', 'backbone.0.body.layer1.2.bn3.weight', 'backbone.0.body.layer1.2.bn3.bias', 'backbone.0.body.layer1.2.bn3.running_mean', 'backbone.0.body.layer1.2.bn3.running_var', 'backbone.0.body.layer2.0.conv1.weight', 'backbone.0.body.layer2.0.bn1.weight', 'backbone.0.body.layer2.0.bn1.bias', 'backbone.0.body.layer2.0.bn1.running_mean', 'backbone.0.body.layer2.0.bn1.running_var', 'backbone.0.body.layer2.0.conv2.weight', 'backbone.0.body.layer2.0.bn2.weight', 'backbone.0.body.layer2.0.bn2.bias', 'backbone.0.body.layer2.0.bn2.running_mean', 'backbone.0.body.layer2.0.bn2.running_var', 'backbone.0.body.layer2.0.conv3.weight', 'backbone.0.body.layer2.0.bn3.weight', 'backbone.0.body.layer2.0.bn3.bias', 'backbone.0.body.layer2.0.bn3.running_mean', 'backbone.0.body.layer2.0.bn3.running_var', 'backbone.0.body.layer2.0.downsample.0.weight', 'backbone.0.body.layer2.0.downsample.1.weight', 'backbone.0.body.layer2.0.downsample.1.bias', 'backbone.0.body.layer2.0.downsample.1.running_mean', 'backbone.0.body.layer2.0.downsample.1.running_var', 'backbone.0.body.layer2.1.conv1.weight', 'backbone.0.body.layer2.1.bn1.weight', 'backbone.0.body.layer2.1.bn1.bias', 'backbone.0.body.layer2.1.bn1.running_mean', 'backbone.0.body.layer2.1.bn1.running_var', 'backbone.0.body.layer2.1.conv2.weight', 'backbone.0.body.layer2.1.bn2.weight', 'backbone.0.body.layer2.1.bn2.bias', 'backbone.0.body.layer2.1.bn2.running_mean', 'backbone.0.body.layer2.1.bn2.running_var', 'backbone.0.body.layer2.1.conv3.weight', 'backbone.0.body.layer2.1.bn3.weight', 'backbone.0.body.layer2.1.bn3.bias', 'backbone.0.body.layer2.1.bn3.running_mean', 'backbone.0.body.layer2.1.bn3.running_var', 'backbone.0.body.layer2.2.conv1.weight', 'backbone.0.body.layer2.2.bn1.weight', 'backbone.0.body.layer2.2.bn1.bias', 'backbone.0.body.layer2.2.bn1.running_mean', 'backbone.0.body.layer2.2.bn1.running_var', 'backbone.0.body.layer2.2.conv2.weight', 'backbone.0.body.layer2.2.bn2.weight', 'backbone.0.body.layer2.2.bn2.bias', 'backbone.0.body.layer2.2.bn2.running_mean', 'backbone.0.body.layer2.2.bn2.running_var', 'backbone.0.body.layer2.2.conv3.weight', 'backbone.0.body.layer2.2.bn3.weight', 'backbone.0.body.layer2.2.bn3.bias', 'backbone.0.body.layer2.2.bn3.running_mean', 'backbone.0.body.layer2.2.bn3.running_var', 'backbone.0.body.layer2.3.conv1.weight', 'backbone.0.body.layer2.3.bn1.weight', 'backbone.0.body.layer2.3.bn1.bias', 'backbone.0.body.layer2.3.bn1.running_mean', 'backbone.0.body.layer2.3.bn1.running_var', 'backbone.0.body.layer2.3.conv2.weight', 'backbone.0.body.layer2.3.bn2.weight', 'backbone.0.body.layer2.3.bn2.bias', 'backbone.0.body.layer2.3.bn2.running_mean', 'backbone.0.body.layer2.3.bn2.running_var', 'backbone.0.body.layer2.3.conv3.weight', 'backbone.0.body.layer2.3.bn3.weight', 'backbone.0.body.layer2.3.bn3.bias', 'backbone.0.body.layer2.3.bn3.running_mean', 'backbone.0.body.layer2.3.bn3.running_var', 'backbone.0.body.layer3.0.conv1.weight', 'backbone.0.body.layer3.0.bn1.weight', 'backbone.0.body.layer3.0.bn1.bias', 'backbone.0.body.layer3.0.bn1.running_mean', 'backbone.0.body.layer3.0.bn1.running_var', 'backbone.0.body.layer3.0.conv2.weight', 'backbone.0.body.layer3.0.bn2.weight', 'backbone.0.body.layer3.0.bn2.bias', 'backbone.0.body.layer3.0.bn2.running_mean', 'backbone.0.body.layer3.0.bn2.running_var', 'backbone.0.body.layer3.0.conv3.weight', 'backbone.0.body.layer3.0.bn3.weight', 'backbone.0.body.layer3.0.bn3.bias', 'backbone.0.body.layer3.0.bn3.running_mean', 'backbone.0.body.layer3.0.bn3.running_var', 'backbone.0.body.layer3.0.downsample.0.weight', 'backbone.0.body.layer3.0.downsample.1.weight', 'backbone.0.body.layer3.0.downsample.1.bias', 'backbone.0.body.layer3.0.downsample.1.running_mean', 'backbone.0.body.layer3.0.downsample.1.running_var', 'backbone.0.body.layer3.1.conv1.weight', 'backbone.0.body.layer3.1.bn1.weight', 'backbone.0.body.layer3.1.bn1.bias', 'backbone.0.body.layer3.1.bn1.running_mean', 'backbone.0.body.layer3.1.bn1.running_var', 'backbone.0.body.layer3.1.conv2.weight', 'backbone.0.body.layer3.1.bn2.weight', 'backbone.0.body.layer3.1.bn2.bias', 'backbone.0.body.layer3.1.bn2.running_mean', 'backbone.0.body.layer3.1.bn2.running_var', 'backbone.0.body.layer3.1.conv3.weight', 'backbone.0.body.layer3.1.bn3.weight', 'backbone.0.body.layer3.1.bn3.bias', 'backbone.0.body.layer3.1.bn3.running_mean', 'backbone.0.body.layer3.1.bn3.running_var', 'backbone.0.body.layer3.2.conv1.weight', 'backbone.0.body.layer3.2.bn1.weight', 'backbone.0.body.layer3.2.bn1.bias', 'backbone.0.body.layer3.2.bn1.running_mean', 'backbone.0.body.layer3.2.bn1.running_var', 'backbone.0.body.layer3.2.conv2.weight', 'backbone.0.body.layer3.2.bn2.weight', 'backbone.0.body.layer3.2.bn2.bias', 'backbone.0.body.layer3.2.bn2.running_mean', 'backbone.0.body.layer3.2.bn2.running_var', 'backbone.0.body.layer3.2.conv3.weight', 'backbone.0.body.layer3.2.bn3.weight', 'backbone.0.body.layer3.2.bn3.bias', 'backbone.0.body.layer3.2.bn3.running_mean', 'backbone.0.body.layer3.2.bn3.running_var', 'backbone.0.body.layer3.3.conv1.weight', 'backbone.0.body.layer3.3.bn1.weight', 'backbone.0.body.layer3.3.bn1.bias', 'backbone.0.body.layer3.3.bn1.running_mean', 'backbone.0.body.layer3.3.bn1.running_var', 'backbone.0.body.layer3.3.conv2.weight', 'backbone.0.body.layer3.3.bn2.weight', 'backbone.0.body.layer3.3.bn2.bias', 'backbone.0.body.layer3.3.bn2.running_mean', 'backbone.0.body.layer3.3.bn2.running_var', 'backbone.0.body.layer3.3.conv3.weight', 'backbone.0.body.layer3.3.bn3.weight', 'backbone.0.body.layer3.3.bn3.bias', 'backbone.0.body.layer3.3.bn3.running_mean', 'backbone.0.body.layer3.3.bn3.running_var', 'backbone.0.body.layer3.4.conv1.weight', 'backbone.0.body.layer3.4.bn1.weight', 'backbone.0.body.layer3.4.bn1.bias', 'backbone.0.body.layer3.4.bn1.running_mean', 'backbone.0.body.layer3.4.bn1.running_var', 'backbone.0.body.layer3.4.conv2.weight', 'backbone.0.body.layer3.4.bn2.weight', 'backbone.0.body.layer3.4.bn2.bias', 'backbone.0.body.layer3.4.bn2.running_mean', 'backbone.0.body.layer3.4.bn2.running_var', 'backbone.0.body.layer3.4.conv3.weight', 'backbone.0.body.layer3.4.bn3.weight', 'backbone.0.body.layer3.4.bn3.bias', 'backbone.0.body.layer3.4.bn3.running_mean', 'backbone.0.body.layer3.4.bn3.running_var', 'backbone.0.body.layer3.5.conv1.weight', 'backbone.0.body.layer3.5.bn1.weight', 'backbone.0.body.layer3.5.bn1.bias', 'backbone.0.body.layer3.5.bn1.running_mean', 'backbone.0.body.layer3.5.bn1.running_var', 'backbone.0.body.layer3.5.conv2.weight', 'backbone.0.body.layer3.5.bn2.weight', 'backbone.0.body.layer3.5.bn2.bias', 'backbone.0.body.layer3.5.bn2.running_mean', 'backbone.0.body.layer3.5.bn2.running_var', 'backbone.0.body.layer3.5.conv3.weight', 'backbone.0.body.layer3.5.bn3.weight', 'backbone.0.body.layer3.5.bn3.bias', 'backbone.0.body.layer3.5.bn3.running_mean', 'backbone.0.body.layer3.5.bn3.running_var', 'backbone.0.body.layer4.0.conv1.weight', 'backbone.0.body.layer4.0.bn1.weight', 'backbone.0.body.layer4.0.bn1.bias', 'backbone.0.body.layer4.0.bn1.running_mean', 'backbone.0.body.layer4.0.bn1.running_var', 'backbone.0.body.layer4.0.conv2.weight', 'backbone.0.body.layer4.0.bn2.weight', 'backbone.0.body.layer4.0.bn2.bias', 'backbone.0.body.layer4.0.bn2.running_mean', 'backbone.0.body.layer4.0.bn2.running_var', 'backbone.0.body.layer4.0.conv3.weight', 'backbone.0.body.layer4.0.bn3.weight', 'backbone.0.body.layer4.0.bn3.bias', 'backbone.0.body.layer4.0.bn3.running_mean', 'backbone.0.body.layer4.0.bn3.running_var', 'backbone.0.body.layer4.0.downsample.0.weight', 'backbone.0.body.layer4.0.downsample.1.weight', 'backbone.0.body.layer4.0.downsample.1.bias', 'backbone.0.body.layer4.0.downsample.1.running_mean', 'backbone.0.body.layer4.0.downsample.1.running_var', 'backbone.0.body.layer4.1.conv1.weight', 'backbone.0.body.layer4.1.bn1.weight', 'backbone.0.body.layer4.1.bn1.bias', 'backbone.0.body.layer4.1.bn1.running_mean', 'backbone.0.body.layer4.1.bn1.running_var', 'backbone.0.body.layer4.1.conv2.weight', 'backbone.0.body.layer4.1.bn2.weight', 'backbone.0.body.layer4.1.bn2.bias', 'backbone.0.body.layer4.1.bn2.running_mean', 'backbone.0.body.layer4.1.bn2.running_var', 'backbone.0.body.layer4.1.conv3.weight', 'backbone.0.body.layer4.1.bn3.weight', 'backbone.0.body.layer4.1.bn3.bias', 'backbone.0.body.layer4.1.bn3.running_mean', 'backbone.0.body.layer4.1.bn3.running_var', 'backbone.0.body.layer4.2.conv1.weight', 'backbone.0.body.layer4.2.bn1.weight', 'backbone.0.body.layer4.2.bn1.bias', 'backbone.0.body.layer4.2.bn1.running_mean', 'backbone.0.body.layer4.2.bn1.running_var', 'backbone.0.body.layer4.2.conv2.weight', 'backbone.0.body.layer4.2.bn2.weight', 'backbone.0.body.layer4.2.bn2.bias', 'backbone.0.body.layer4.2.bn2.running_mean', 'backbone.0.body.layer4.2.bn2.running_var', 'backbone.0.body.layer4.2.conv3.weight', 'backbone.0.body.layer4.2.bn3.weight', 'backbone.0.body.layer4.2.bn3.bias', 'backbone.0.body.layer4.2.bn3.running_mean', 'backbone.0.body.layer4.2.bn3.running_var'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util.misc as utils\n",
    "from datasets.open_world_eval import OWEvaluator\n",
    "from datasets.panoptic_eval import PanopticEvaluator\n",
    "from datasets.data_prefetcher import data_prefetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "criterion.eval()\n",
    "metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "header = 'Test:'\n",
    "iou_types = tuple(k for k in ('segm', 'bbox') if k in postprocessors.keys())\n",
    "coco_evaluator = OWEvaluator(base_ds, iou_types, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for samples, targets in metric_logger.log_every(data_loader, 10, header):\n",
    "    samples = samples.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    outputs = model(samples)\n",
    "    orig_target_sizes = torch.stack([t[\"orig_size\"] for t in targets], dim=0)\n",
    "    results = postprocessors['bbox'](outputs, orig_target_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update XML files\n",
    "Also in script `update_xml.py` - still needs a main.\n",
    "\n",
    "### Add missing tags to XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def replace_in_file(file_path, pattern, replacement):\n",
    "    with open(file_path, 'r+') as file:\n",
    "        file_content = file.read()\n",
    "        file_content = re.sub(pattern, replacement, file_content)\n",
    "        file.seek(0)\n",
    "        file.write(file_content)\n",
    "        file.truncate()\n",
    "\n",
    "def replace_in_all_files(directory, pattern, replacement):\n",
    "    for foldername, subfolders, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            replace_in_file(file_path, pattern, replacement)\n",
    "\n",
    "directory = \"./data/OWOD/Annotations/\"\n",
    "pattern = \"/name>\\n        <bndbox>\"\n",
    "replacement = \"/name>\\n        <truncated>0</truncated>\\n        <difficult>0</difficult>\\n        <bndbox>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_in_all_files(directory, pattern, replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update annotation fiel with new path and new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "dict_csv = \"./data/OWOD/filename_map.csv\"\n",
    "xml_dir = \"./data/OWOD/Annotations/\"\n",
    "new_path_prefix = \"/home/sabrina/code/PROB/data/OWOD/ImageSets/\"\n",
    "\n",
    "def main(dict_csv, xml_dir, new_path_prefix):\n",
    "    logging.basicConfig(filename='xml_update.log', level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(dict_csv)\n",
    "    name_dict = df.set_index('old_name')['new_name'].to_dict()\n",
    "\n",
    "    xml_files = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "\n",
    "    for xml_file in tqdm(xml_files, desc=\"Updating XML files\"):\n",
    "        tree = ET.parse(os.path.join(xml_dir, xml_file))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for elem in root.iter():\n",
    "            try:\n",
    "                if elem.tag == 'filename':\n",
    "                    old_filename = elem.text.split('.')[0]\n",
    "                    file_extension = elem.text.split('.')[1]\n",
    "\n",
    "                    new_filename = f\"{name_dict[old_filename]}.{file_extension}\"\n",
    "                    new_path = f\"{new_path_prefix}{new_filename}\"\n",
    "\n",
    "                    elem.text = new_filename\n",
    "\n",
    "                if elem.tag == 'path':\n",
    "                    elem.text = new_path\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing XML file {xml_file}: {e}\")\n",
    "                pass\n",
    "\n",
    "        tree.write(os.path.join(xml_dir, xml_file))                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# main(dict_csv, xml_dir, new_path_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (480740770.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    --pretrain \"/exps/MOWODB/PROB/t1/checkpoint0040.pth\"\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!python -u main_open_world.py \\ \n",
    "--eval \\\n",
    "--pretrain \"/exps/MOWODB/PROB/t1/checkpoint0040.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
